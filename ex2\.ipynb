{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4 - Dealing with noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Data Preparation Course at UCU, 2019_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB\n",
    "\n",
    "__1) Which programming languages to use?__\n",
    "\n",
    "We recommend to use Python for this task, but if you find working library alternatives for the algorithms we\n",
    "use in this assignment in R, you are free to work with that as well.\n",
    "\n",
    "__2) What libraries/packages to use?__\n",
    "\n",
    "You are free to choose any appropriate libraries (good choice would be __pandas__, __numpy__,\n",
    "__scicit-learn__).\n",
    "\n",
    "__3) How to summarize my homework?__\n",
    "\n",
    "The best way is to create an Jupyter/R notebook with code and explanations for each strategy. In case you\n",
    "are not familiar with these tools, you can create a Python/R scripts and write explanations as comments.\n",
    "However, we strongly recommend you to use Jupyter/R notebooks, as those are #1 tools in applied data\n",
    "analysis nowadays.\n",
    "\n",
    "__4) Useful links__\n",
    "\n",
    "1. [Deaing with Noisy Data in Data Science.](https://medium.com/analytics-vidhya/dealing-with-noisy-data-in-data-science-e177a4e32621)\n",
    "2. [Decision trees in Scikit-learn.](https://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "In this homework you will investigate the impact of different types of noise on the accuracy of classification\n",
    "model based on __<font color=\"black\">[(Census Income dataset)](https://archive.ics.uci.edu/ml/datasets/Census+Income)</font>__. Noise is an unavoidable problem which affects all stages of Data Mining process, so it is extremely important to learn how to deal with the noise in the most appropriate way. \n",
    "\n",
    "### __1) Logistic regression.__\n",
    "\n",
    "Similar to previous assignment, you’ll have to train multiple logistic regression models. We encourage you to use provided jupyter notebook with working template of logistic regression for Census dataset. Please remember that LR is not the main topic of this assignment, so do not bother yourself tuning your models. The purpose of this assignment is to investigate the negative impact of noise in your dataset on the accuracy of classification and learn basic methods of dealing with problems of this type.\n",
    "\n",
    "Regarding missing values in the dataset - you need to impute them using __global most common substitution strategy__ from the previous assignment.\n",
    "\n",
    "__Treat dataset you obtain after missing values imputation as an original one. All further\n",
    "modification in this homework perform on this dataset, not on the one you have before missing\n",
    "values imputation.__\n",
    "\n",
    "__1.1.__ Train original logistic regression model provided in jupyter notebook. Save values of train and test\n",
    "classification accuracy scores for future comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"age\",\"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "                   \"occupation\", \"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\n",
    "                   \"hours-per-week\",\"native-country\",\"profit\"]\n",
    "                \n",
    "df = pd.read_csv(\"adult.data\", header=None, names=column_names, index_col=False)\n",
    "test_df = pd.read_csv(\"adults.test\", header=None, names=column_names, index_col=False, skiprows=1)\n",
    "indexes=[\"attribute noise\",\"target noise\", \"no noise\"]\n",
    "score_table = pd.DataFrame(columns=[\"1%\",\"5%\",\"10%\",\"20%\",\"30%\",\"40%\",\"50%\"],index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  profit  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dff, train=False):\n",
    "    df=dff.copy()\n",
    "    df[(df[\"workclass\"] == \" ?\") | (df[\"occupation\"] == \" ?\") | (df[\"native-country\"] == \" ?\")] = df[(df[\"workclass\"] == \" ?\") | (df[\"occupation\"] == \" ?\") | (df[\"native-country\"] == \" ?\") ].replace(\" ?\",np.nan)\n",
    "    df.loc[df[\"capital-gain\"] == 0, \"capital-gain\"] = df[\"capital-gain\"].mean()\n",
    "    df.loc[df[\"capital-loss\"] == 0, \"capital-loss\"] = df[\"capital-loss\"].mean()\n",
    "    df.fillna(df.mean(),inplace=True)\n",
    "    df.drop([\"education\",\"marital-status\",\"fnlwgt\"],axis=1,inplace=True)  \n",
    "    if train:\n",
    "        tmp = {\" <=50K\":0, \" >50K\":1}\n",
    "    else:\n",
    "        tmp = {\" <=50K.\":0, \" >50K.\":1}\n",
    "    df[\"profit\"] = df[\"profit\"].map(lambda x: tmp[x])\n",
    "    df.loc[df[\"native-country\"].isin([' United-States', ' Canada', ' South']), \"native-country\"] = \"North America\"\n",
    "    df.loc[df[\"native-country\"].isin([' Mexico',' Dominican-Republic',' Guatemala',' Trinadad&Tobago', ' Columbia',' El-Salvador', ' Ecuador', ' Philippines',' Honduras',' Outlying-US(Guam-USVI-etc)', ' Haiti', ' Puerto-Rico', \" Nicaragua\", 'Nicaragua', ' Peru', ' Cuba',' Jamaica']), \"native-country\"] = \"Latin America\"\n",
    "    df.loc[df[\"native-country\"].isin([' Taiwan', ' Hong', ' Thailand',' Japan', ' Cambodia', ' India',' Iran',' Vietnam',' China', ' Laos']), \"native-country\"] = \"Asia\"\n",
    "    df.loc[df[\"native-country\"].isin([' Italy',' Greece',' Holand-Netherlands', ' Poland',' Germany',' England',' Ireland',' Hungary',' France', ' Yugoslavia',' Scotland', ' France',' Portugal', \" Netherlands\"]), \"native-country\"] = \"Europe\"\n",
    "    df = pd.get_dummies(df)\n",
    "    pr = df[\"profit\"]\n",
    "    df.drop([\"profit\"],axis=1,inplace=True)\n",
    "    return df,pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_data(df,True)\n",
    "X_tst, y_tst = prepare_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8522203795835637\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver=\"liblinear\")\n",
    "lr.fit(X,y)\n",
    "print(lr.score(X_tst, y_tst))\n",
    "score_table.loc[\"no noise\",:] = lr.score(X_tst, y_tst) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2) [1pt] Misclassification noise.__\n",
    "\n",
    "__2.1.__ Introduce misclassification in your dataset. Randomly flip $n\\%$ of the target variable (‘y’) values. Try $n = (1, 5, 10, 20)$. Perform this process __only in train dataset__. Leave test dataset unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "percents = [1,5,10,20,30,40,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNoise(df_col, perc, chng=None): # if none than negate\n",
    "    indexes = df_col.sample(int(df_col.shape[0]*0.01*perc)).index\n",
    "    if chng is None:\n",
    "        chng = {val:0 for val in df_col.iloc[indexes]}\n",
    "    return pd.concat([df_col.iloc[indexes].map(lambda x: chng[x]), df_col.iloc[~df_col.index.isin(indexes)]]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ar = list()\n",
    "for n in [1,5,10,20,30,40,50]:\n",
    "    y_ar.append(addNoise(y, n,{0:1,1:0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.2.__ For each $n$ train separate model. Record train and test accuracy for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8463192162402875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8531416989128432"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8165596879702712"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8517904305632332"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7785080310801266"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8484736809778269"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7038481619114892"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8441741907745225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6372347286631246"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8425158159818193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5721261632013759"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8328112523800749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5112557968121372"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4895890915791413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inx,y_ in enumerate(y_ar):\n",
    "    lr = LogisticRegression(solver=\"liblinear\")\n",
    "    lr.fit(X,y_)\n",
    "    \n",
    "    display(lr.score(X,y_), lr.score(X_tst,y_tst))\n",
    "    score_table.loc[\"target noise\",str(percents[inx])+\"%\"] = lr.score(X_tst,y_tst)    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.3.__ What is the highest safe fraction (approximately) of misclassified examples? (by ‘safe’, we mean fraction\n",
    "of misclassified examples with which difference of accuracies between original and misclassified model does\n",
    "not exceed 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be that 40% of wrong y labels is the threshold, when test drops more than 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3) [1pt] Attribute noise.__\n",
    "\n",
    "__3.0.__ For $n = (1,5,10,20)$ create datasets with different levels of attribute noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.1.__ Introduce attribute noise to the __age__ column. Randomly negate $n\\%$ of the values of this attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [df.copy() for _ in range(7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for inx,ds in enumerate(datasets):\n",
    "    ds[\"age\"] = addNoise(ds[\"age\"],percents[inx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.2.__ Introduce attribute noise to the __education_num__ column. Randomly replace $n\\%$ of the values of this attribute with random large numbers in range $[20,100]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inx, ds in enumerate(datasets):\n",
    "    ds[\"education-num\"] = addNoise(ds[\"education-num\"],percents[inx], {num:np.random.randint(20,100) for num in range(20)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.3.__ Introduce attirute noise to the __race__ column. Randomly replace $n\\%$ of the values of this attribute with\n",
    "any other random race from the set of existing races."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chng_r = {race:np.random.choice([other_race for other_race in df[\"race\"].unique() if other_race != race]) for race in df[\"race\"].unique()}\n",
    "for inx, ds in enumerate(datasets):\n",
    "    ds[\"race\"] = addNoise(ds[\"race\"],percents[inx], chng_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.4.__ For each $n$ train separate model. Record train and test accuracy for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8439544240041768 0.8460782507217002\n",
      "0.8421117287552594 0.8452183526810393\n",
      "0.8435244617794294 0.8452183526810393\n",
      "0.8444150978164061 0.8461396720103188\n",
      "0.8431559227296459 0.8455254591241325\n",
      "0.8439851355916588 0.8455868804127511\n",
      "0.843831577654249 0.8458325655672256\n"
     ]
    }
   ],
   "source": [
    "for inx,ds in enumerate(datasets):\n",
    "    X,y = prepare_data(ds,train=True)\n",
    "    \n",
    "    lr = LogisticRegression(solver=\"liblinear\")\n",
    "    lr.fit(X,y)\n",
    "    \n",
    "    print(lr.score(X,y), lr.score(X_tst,y_tst))\n",
    "    score_table.loc[\"attribute noise\",str(percents[inx])+\"%\"] = lr.score(X_tst,y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.5.__ Quantify the degradation of the model after introducing each new level of noise to its attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't(almost). I think it happens because of small feature correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __4) [1pt] Impact comparison.__\n",
    "\n",
    "__4.1.__ Build a table to compare accuracy of the model on the original dataset with models based on datasets\n",
    "with different types and levels of noise introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1%</th>\n",
       "      <th>5%</th>\n",
       "      <th>10%</th>\n",
       "      <th>20%</th>\n",
       "      <th>30%</th>\n",
       "      <th>40%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>attribute noise</td>\n",
       "      <td>0.846078</td>\n",
       "      <td>0.845218</td>\n",
       "      <td>0.845218</td>\n",
       "      <td>0.84614</td>\n",
       "      <td>0.845525</td>\n",
       "      <td>0.845587</td>\n",
       "      <td>0.845833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>target noise</td>\n",
       "      <td>0.853142</td>\n",
       "      <td>0.85179</td>\n",
       "      <td>0.848474</td>\n",
       "      <td>0.844174</td>\n",
       "      <td>0.842516</td>\n",
       "      <td>0.832811</td>\n",
       "      <td>0.489589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>no noise</td>\n",
       "      <td>0.85222</td>\n",
       "      <td>0.85222</td>\n",
       "      <td>0.85222</td>\n",
       "      <td>0.85222</td>\n",
       "      <td>0.85222</td>\n",
       "      <td>0.85222</td>\n",
       "      <td>0.85222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       1%        5%       10%       20%       30%       40%  \\\n",
       "attribute noise  0.846078  0.845218  0.845218   0.84614  0.845525  0.845587   \n",
       "target noise     0.853142   0.85179  0.848474  0.844174  0.842516  0.832811   \n",
       "no noise          0.85222   0.85222   0.85222   0.85222   0.85222   0.85222   \n",
       "\n",
       "                      50%  \n",
       "attribute noise  0.845833  \n",
       "target noise     0.489589  \n",
       "no noise          0.85222  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.2.__ What has greater impact on the accuracy of the model: class or attribute noise? How would you explain\n",
    "it? (4-5 sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class influences model accuracy more, because features can either be correlated with target or not(have a big impact or small), but target obviously correlated with itself so changing parameters should ruin the pattern, that model need to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.3.__ What kind of noise would you address first? Why? (2-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd first try to take care of class noise, if it is possible cause spoils the results more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __5) [2pt] Misclassification noise elimination.__\n",
    "\n",
    "__5.1.__ Use training dataset with 10% of misclassified instances which you obtained in __Task 2__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = y_ar[2]\n",
    "train_X = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n",
       "                9,\n",
       "            ...\n",
       "            32551, 32552, 32553, 32554, 32555, 32556, 32557, 32558, 32559,\n",
       "            32560],\n",
       "           dtype='int64', length=32561)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.2.__ Apply Cross-Validated Committees Filter algorithm to identify and fix mislabled instances in this dataset.\n",
    "\n",
    "• You can read the full description of this algorithm in “Data Preprocessing In Data Mining” by S. Garcia,\n",
    "J. Luengo and F. Herrera [page 117, Section 5.3.2].\n",
    "\n",
    "• Use scikit-learn utilities to create and train Decision Tree classifiers for this algorithm. You can read\n",
    "more about them in [__(Decision Trees)__](https://scikit-learn.org/stable/modules/tree.html).\n",
    "\n",
    "• Use $\\Gamma = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "kf = KFold(n_splits=5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for train,test in kf.split(train_X):\n",
    "    tr_X, ts_X = train_X.iloc[train,:], train_X.iloc[test,:]\n",
    "    tr_y, ts_y = train_y[train], train_y[test]\n",
    "    \n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(tr_X,tr_y)\n",
    "    \n",
    "    models.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2953\n"
     ]
    }
   ],
   "source": [
    "votes = {inx:0 for inx in train_y.index}\n",
    "for model in models:\n",
    "#     pred = pd.Series(model.predict(train_X), index=train_y.index)\n",
    "    diff = pd.concat([pred,train_y]).drop_duplicates(keep=False)\n",
    "    \n",
    "    for i in train_y.index:\n",
    "        if train_y[i] != pred[i]:\n",
    "            votes[i] += 1\n",
    "\n",
    "diff = {inx:votes[inx] for inx in votes.keys() if votes[inx] > 2}\n",
    "print(len(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.3.__ What percent of mislabled records you fixed using this method? Is it possible to do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did manages to find 2953 of wrong data out of 3256 possible, though I'm not sure all of them were classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.4.__ Compare the accuracy of the classifier after elimination of mislabeled instances with its accuracy before\n",
    "this procedure was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.drop([i for i in diff.keys()], inplace=True)\n",
    "train_y.drop([i for i in diff.keys()], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7904424181019926\n"
     ]
    }
   ],
   "source": [
    "tr_X, ts_X = train_test_split(train_X, test_size=0.2,shuffle=False)\n",
    "tr_y, ts_y = train_test_split(train_y, test_size=0.2,shuffle=False)\n",
    "\n",
    "lg = LogisticRegression(solver=\"liblinear\")\n",
    "lg.fit(tr_X, tr_y)\n",
    "print(lg.score(ts_X, ts_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like it is even worse now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
